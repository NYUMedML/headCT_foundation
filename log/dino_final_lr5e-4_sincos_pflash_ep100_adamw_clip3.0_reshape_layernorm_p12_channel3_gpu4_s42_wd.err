W0112 02:05:19.436621 23456247981568 torch/distributed/run.py:779] 
W0112 02:05:19.436621 23456247981568 torch/distributed/run.py:779] *****************************************
W0112 02:05:19.436621 23456247981568 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0112 02:05:19.436621 23456247981568 torch/distributed/run.py:779] *****************************************
/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/monai-dev/lib/python3.8/site-packages/ignite/handlers/checkpoint.py:17: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import ZeroRedundancyOptimizer
/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/monai-dev/lib/python3.8/site-packages/ignite/handlers/checkpoint.py:17: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import ZeroRedundancyOptimizer
/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/monai-dev/lib/python3.8/site-packages/ignite/handlers/checkpoint.py:17: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import ZeroRedundancyOptimizer
/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/monai-dev/lib/python3.8/site-packages/ignite/handlers/checkpoint.py:17: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import ZeroRedundancyOptimizer
`torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
`torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
`torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
wandb: Currently logged in as: notody. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /gpfs/data/denizlab/Users/hh2740/git_backups/HeadCT-Foundation/slurm_submit/wandb/run-20250112_020735-a7c3kyw2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dino_final_lr5e-4_sincos_pflash_ep100_adamw_clip3.0_reshape_layernorm_p12_channel3_gpu4_s42_wd
wandb: ‚≠êÔ∏è View project at https://wandb.ai/notody/monai-test
wandb: üöÄ View run at https://wandb.ai/notody/monai-test/runs/a7c3kyw2
`torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
[rank2]:[W112 02:14:31.550157643 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W112 02:14:37.787551010 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W112 02:15:19.730751884 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W112 02:15:40.140914285 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)
wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)
W0113 04:15:14.355346 23456247981568 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1484616 closing signal SIGTERM
W0113 04:15:14.366907 23456247981568 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1484617 closing signal SIGTERM
W0113 04:15:14.367233 23456247981568 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1484619 closing signal SIGTERM
W0113 04:15:44.367659 23456247981568 torch/distributed/elastic/multiprocessing/api.py:875] Unable to shutdown process 1484616 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
W0113 04:16:20.929381 23456247981568 torch/distributed/elastic/multiprocessing/api.py:875] Unable to shutdown process 1484619 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
E0113 04:16:20.944998 23456247981568 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: -9) local_rank: 2 (pid: 1484618) of binary: /gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/bin/python
Traceback (most recent call last):
  File "/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/lib/python3.8/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/lib/python3.8/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
../main_pretrain_dino.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-01-13_04:15:14
  host      : rc-4001.cm.cluster
  rank      : 2 (local_rank: 2)
  exitcode  : -9 (pid: 1484618)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 1484618
========================================================
slurmstepd-rc-4001: error: Detected 5 oom_kill events in StepId=58054222.batch. Some of the step tasks have been OOM Killed.
