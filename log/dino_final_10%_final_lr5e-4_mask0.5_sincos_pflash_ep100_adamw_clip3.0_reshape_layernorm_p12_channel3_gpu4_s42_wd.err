W0110 02:30:08.855250 23456247981568 torch/distributed/run.py:779] 
W0110 02:30:08.855250 23456247981568 torch/distributed/run.py:779] *****************************************
W0110 02:30:08.855250 23456247981568 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0110 02:30:08.855250 23456247981568 torch/distributed/run.py:779] *****************************************
/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/monai-dev/lib/python3.8/site-packages/ignite/handlers/checkpoint.py:17: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import ZeroRedundancyOptimizer
/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/monai-dev/lib/python3.8/site-packages/ignite/handlers/checkpoint.py:17: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import ZeroRedundancyOptimizer
/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/monai-dev/lib/python3.8/site-packages/ignite/handlers/checkpoint.py:17: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import ZeroRedundancyOptimizer
/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/monai-dev/lib/python3.8/site-packages/ignite/handlers/checkpoint.py:17: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  from torch.distributed.optim import ZeroRedundancyOptimizer
`torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
`torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
`torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
wandb: Currently logged in as: notody. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /gpfs/data/denizlab/Users/hh2740/git_backups/HeadCT-Foundation/slurm_submit/wandb/run-20250110_023132-ua94inuw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dino_final_lr5e-4_sincos_pflash_ep100_adamw_clip3.0_reshape_layernorm_p12_channel3_gpu4_s42_wd
wandb: ‚≠êÔ∏è View project at https://wandb.ai/notody/monai-test
wandb: üöÄ View run at https://wandb.ai/notody/monai-test/runs/ua94inuw
`torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.
[rank1]:[W110 02:34:52.385427204 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W110 02:35:12.173645780 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W110 02:37:09.682071199 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W110 02:37:47.121453588 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
W0110 05:22:10.386105 23456247981568 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3415819 closing signal SIGTERM
W0110 05:22:10.398189 23456247981568 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3415821 closing signal SIGTERM
W0110 05:22:10.398357 23456247981568 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 3415822 closing signal SIGTERM
E0110 05:22:33.583935 23456247981568 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: -9) local_rank: 1 (pid: 3415820) of binary: /gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/bin/python
Traceback (most recent call last):
  File "/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/lib/python3.8/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/lib/python3.8/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/gpfs/data/denizlab/Users/hh2740/miniconda3/envs/head_ct/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
../main_pretrain_dino.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-01-10_05:22:10
  host      : a100-4015.cm.cluster
  rank      : 1 (local_rank: 1)
  exitcode  : -9 (pid: 3415820)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 3415820
========================================================
slurmstepd-a100-4015: error: Detected 1 oom_kill event in StepId=58034291.batch. Some of the step tasks have been OOM Killed.
